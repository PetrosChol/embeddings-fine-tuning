{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_markdown"
      },
      "source": [
        "# Fine-Tuning a Sentence Transformer Model for Financial Text\n",
        "\n",
        "This notebook demonstrates a best-practice workflow for fine-tuning a pre-trained sentence-transformer model on a specialized domain. In this case, a financial question-and-context dataset.\n",
        "\n",
        "The key steps are:\n",
        "1.  **Setup**: Install necessary libraries.\n",
        "2.  **Data Preparation**: Load the dataset and create a robust three-way split (train, validation and test).\n",
        "3.  **Model Configuration**: Define the training arguments, loss function, and evaluators.\n",
        "4.  **Training**: Run the training process, using the validation set to monitor performance and save the best model.\n",
        "5.  **Evaluation**: Perform a final, unbiased evaluation on the held-out test set.\n",
        "6.  **Comparison**: Compare the performance of the fine-tuned model against the original pre-trained model to quantify the improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_markdown"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, we install the required Python libraries. `sentence-transformers` is the core library for this task, and we also need `datasets` for data handling and `transformers` as a dependency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEsG2adLDFKV"
      },
      "outputs": [],
      "source": [
        "!pip install torch sentence-transformers datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_loading_markdown"
      },
      "source": [
        "## 2. Data Loading and Initial Processing\n",
        "\n",
        "We will load the `philschmid/finanical-rag-embedding-dataset` from the Hugging Face Hub. This dataset contains pairs of financial questions and their corresponding context paragraphs, which is ideal for our retrieval task.\n",
        "\n",
        "We also load the pre-trained `sentence-transformers/all-MiniLM-L6-v2` model. This is a strong, general-purpose model that we will specialize for our financial domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s10-7rpf-EQb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the base model from the Hugging Face Hub.\n",
        "model = SentenceTransformer(\n",
        "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Load the financial dataset.\n",
        "dataset = load_dataset(\"philschmid/finanical-rag-embedding-dataset\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_splitting_markdown"
      },
      "source": [
        "## 3. Creating a Robust Train, Validation, and Test Split\n",
        "\n",
        "This is a critical step for reliable model evaluation. We will split our data into three distinct sets:\n",
        "- **Training Set (81%)**: The data the model learns from.\n",
        "- **Validation Set (9%)**: Data held out from training, used to check the model's performance at the end of each epoch. This helps us find the best model and prevent overfitting.\n",
        "- **Test Set (10%)**: Data that is completely untouched during training and validation. It's used only once at the end to get a final, unbiased measure of the model's performance.\n",
        "\n",
        "We will save these splits to JSON files for easy access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeGJrFCzO0Pt"
      },
      "outputs": [],
      "source": [
        "# Rename the columns to the format expected by the trainer: 'anchor' for the query and 'positive' for the relevant document.\n",
        "dataset = dataset.rename_column(\"question\", \"anchor\")\n",
        "dataset = dataset.rename_column(\"context\", \"positive\")\n",
        "\n",
        "# Add a unique ID to each row, which is useful for creating our evaluation dictionaries.\n",
        "dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
        "\n",
        "# First, split off the test set (10% of the total data).\n",
        "train_val_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "# Next, split the remaining 90% into a new training set and a validation set.\n",
        "# The validation set will be 10% of this remaining data.\n",
        "train_dataset_final = train_val_dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "# Save the three final datasets to disk.\n",
        "train_dataset_final[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
        "train_dataset_final[\"test\"].to_json(\"validation_dataset.json\", orient=\"records\") # This is our validation set\n",
        "train_val_dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\") # This is our final test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLEL5FtvsRS6"
      },
      "outputs": [],
      "source": [
        "# Load the splits from the JSON files we just created.\n",
        "train_dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n",
        "validation_dataset = load_dataset(\"json\", data_files=\"validation_dataset.json\", split=\"train\")\n",
        "test_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
        "\n",
        "# The InformationRetrievalEvaluator needs a 'corpus' of all possible documents to search from.\n",
        "# We create this by combining all three splits.\n",
        "corpus_dataset = concatenate_datasets([train_dataset, validation_dataset, test_dataset])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eval_data_prep_markdown"
      },
      "source": [
        "## 4. Preparing Data for the Evaluator\n",
        "\n",
        "The `InformationRetrievalEvaluator` requires the data to be in a specific dictionary format:\n",
        "- `corpus`: A dictionary mapping a document ID to the document text.\n",
        "- `queries`: A dictionary mapping a query ID to the query text.\n",
        "- `relevant_docs`: A dictionary mapping a query ID to a list of relevant document IDs.\n",
        "\n",
        "We will create these dictionaries for both our validation set and our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMa2IPzKsdII"
      },
      "outputs": [],
      "source": [
        "# Create the corpus dictionary from the combined dataset.\n",
        "corpus = dict(\n",
        "    zip(corpus_dataset[\"id\"], corpus_dataset[\"positive\"])\n",
        ")\n",
        "\n",
        "# Create the queries and relevant documents dictionaries for the VALIDATION set.\n",
        "val_queries = dict(\n",
        "    zip(validation_dataset[\"id\"], validation_dataset[\"anchor\"])\n",
        ")\n",
        "val_relevant_docs = {}\n",
        "for q_id in val_queries:\n",
        "    val_relevant_docs[q_id] = [q_id]\n",
        "\n",
        "# Create the queries and relevant documents dictionaries for the TEST set.\n",
        "test_queries = dict(\n",
        "    zip(test_dataset[\"id\"], test_dataset[\"anchor\"])\n",
        ")\n",
        "test_relevant_docs = {}\n",
        "for q_id in test_queries:\n",
        "    test_relevant_docs[q_id] = [q_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_markdown"
      },
      "source": [
        "## 5. Model and Training Configuration\n",
        "\n",
        "Now we configure the components needed for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loss_markdown"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "We use `MultipleNegativesRankingLoss`, a highly effective loss function for this type of task. For each training sample (an anchor-positive pair), it treats all other positive samples within the same batch as negative examples. This creates a rich set of challenging negatives for the model to learn from, improving its ability to distinguish between similar documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVfrRSNXXq5N"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
        "\n",
        "loss = MultipleNegativesRankingLoss(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "args_markdown"
      },
      "source": [
        "### Training Arguments\n",
        "\n",
        "We define all the training parameters using `SentenceTransformerTrainingArguments`. This is where we set the number of epochs, batch sizes, learning rate, and evaluation strategy.\n",
        "\n",
        "Crucially, we set:\n",
        "- `eval_strategy=\"epoch\"`: to run evaluation at the end of each epoch.\n",
        "- `load_best_model_at_end=True`: to ensure the trainer reloads the model weights from the epoch that had the best performance on the validation set.\n",
        "- `metric_for_best_model=\"eval_validation_cosine_ndcg@10\"`: This tells the trainer that the \"best\" model is the one with the highest `ndcg@10` score on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUnigpG4Y5L9"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformerTrainingArguments\n",
        "from sentence_transformers.training_args import BatchSamplers\n",
        "\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    # A name for the output directory.\n",
        "    output_dir=\"all-MiniLM-L6-v2-financial\",\n",
        "    # The number of training epochs.\n",
        "    num_train_epochs=6,\n",
        "    # The batch size for the training dataloader.\n",
        "    per_device_train_batch_size=32,\n",
        "    # The batch size for the evaluation dataloader.\n",
        "    per_device_eval_batch_size=16,\n",
        "    # The learning rate.\n",
        "    learning_rate=2e-5,\n",
        "    # Use a cosine learning rate scheduler.\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    # Use a fused AdamW optimizer for faster training.\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    # Use mixed precision training for a speedup.\n",
        "    fp16=True,\n",
        "    # This loss benefits from not having duplicates in the batch.\n",
        "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
        "    # Run evaluation at the end of each epoch.\n",
        "    eval_strategy=\"epoch\",\n",
        "    # Save the model at the end of each epoch.\n",
        "    save_strategy=\"epoch\",\n",
        "    # Only keep the last 3 saved models.\n",
        "    save_total_limit=3,\n",
        "    # When training is finished, load the best model found during training.\n",
        "    load_best_model_at_end=True,\n",
        "    # The metric to use to compare models and select the best one.\n",
        "    metric_for_best_model=\"eval_validation_cosine_ndcg@10\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluator_markdown"
      },
      "source": [
        "### Validation Evaluator\n",
        "\n",
        "We create an `InformationRetrievalEvaluator` that will be used by the trainer to assess the model's performance on the **validation set** after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1pbwN4Siyvg"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "from sentence_transformers.util import cos_sim\n",
        "\n",
        "# The evaluator passed to the trainer will use the validation set.\n",
        "validation_evaluator = InformationRetrievalEvaluator(\n",
        "    queries=val_queries,\n",
        "    corpus=corpus,\n",
        "    relevant_docs=val_relevant_docs,\n",
        "    score_functions={\"cosine\": cos_sim},\n",
        "    name=\"validation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trainer_markdown"
      },
      "source": [
        "### Assembling the Trainer\n",
        "\n",
        "Finally, we bring all the components together in the `SentenceTransformerTrainer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7HTaxpPtEJ9"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformerTrainer\n",
        "\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset.select_columns(\n",
        "        [\"anchor\", \"positive\"]\n",
        "    ),\n",
        "    loss=loss,\n",
        "    evaluator=validation_evaluator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_markdown"
      },
      "source": [
        "## 6. Training the Model\n",
        "\n",
        "Now we can start the training process. The trainer will display a table showing the performance on the validation set after each epoch. Notice how it tracks the `eval_validation_cosine_ndcg@10` metric we specified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onjrliWBmk8w"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNoc_aZCthV8"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final_eval_markdown"
      },
      "source": [
        "## 7. Final Evaluation on the Test Set\n",
        "\n",
        "Training is complete. Because we set `load_best_model_at_end=True`, the `trainer` object now holds the model from the epoch with the best validation score.\n",
        "\n",
        "Now, we perform the final evaluation on the held-out **test set** to get an unbiased measure of its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZD6sRmTGv9D-"
      },
      "outputs": [],
      "source": [
        "# Load the best model that was saved during training.\n",
        "final_model = SentenceTransformer(\n",
        "    args.output_dir, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Create a new evaluator specifically for the test set.\n",
        "test_evaluator = InformationRetrievalEvaluator(\n",
        "        queries=test_queries,\n",
        "        corpus=corpus,\n",
        "        relevant_docs=test_relevant_docs,\n",
        "        score_functions={\"cosine\": cos_sim},\n",
        "        name=\"test\"\n",
        "    )\n",
        "\n",
        "# Evaluate the model on the test set.\n",
        "test_results = test_evaluator(final_model)\n",
        "print(\"Final results on the test set:\")\n",
        "print(test_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comparison_markdown"
      },
      "source": [
        "## 8. Comparison and Conclusion\n",
        "\n",
        "The final step is to compare the performance of our fine-tuned model against the original, off-the-shelf `all-MiniLM-L6-v2` model. This will clearly demonstrate the value of fine-tuning on our domain-specific data.\n",
        "\n",
        "We run the original model through the same `test_evaluator` and then display the results side-by-side in a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsWoAe87wh_R"
      },
      "outputs": [],
      "source": [
        "# Load the original, pre-trained model again.\n",
        "original_model = SentenceTransformer(\n",
        "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Evaluate the original model on the same test set.\n",
        "print(\"Evaluating the original model...\")\n",
        "original_model_results = test_evaluator(original_model)\n",
        "\n",
        "print(\"\\n--- Evaluation Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQKPKYIXwm8f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a dictionary to hold both sets of results.\n",
        "comparison_data = {\n",
        "    \"Original Model\": original_model_results,\n",
        "    \"Fine-Tuned Model\": test_results\n",
        "}\n",
        "\n",
        "# Convert to a Pandas DataFrame for easy viewing.\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Calculate the percentage improvement.\n",
        "df_comparison['Improvement'] = (\n",
        "    (df_comparison['Fine-Tuned Model'] - df_comparison['Original Model']) / df_comparison['Original Model']\n",
        ") * 100\n",
        "\n",
        "# Format the improvement column to show as a percentage.\n",
        "df_comparison['Improvement'] = df_comparison['Improvement'].map('{:.2f}%'.format)\n",
        "\n",
        "print(\"--- Performance Comparison on the Test Set ---\")\n",
        "print(df_comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_markdown"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "The comparison table clearly shows a significant improvement across all metrics. For example, the **`test_cosine_ndcg@10`** score, a key measure of ranking quality, improved by over **10%**. This demonstrates that fine-tuning has successfully adapted the model to the nuances of financial language, making it a much more effective retrieval tool for this specific domain."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}